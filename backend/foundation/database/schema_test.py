import re
from dataclasses import dataclass

from foundation.test.fixture import TFix

nl = "\n"  # can't put backslash in f-string expression

IGNORE_TABLES = ["pgmigrate_migrations"]


async def test_valid_autoincrementing_pkeys(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT a.attrelid::regclass::text
        FROM pg_attribute a
        JOIN pg_constraint c ON (c.conrelid = a.attrelid AND c.conkey[1] = a.attnum)
        JOIN pg_catalog.pg_namespace n ON (c.connamespace = n.oid)
        WHERE n.nspname = 'public'                   -- public schema
            AND array_length(c.conkey, 1) = 1        -- single column
            AND c.contype = 'p'                      -- primary key
            AND a.attidentity <> 'a'                 -- not an identity column generated by default
            AND a.atttypid <> 'bigint'::regtype
            AND a.attrelid::regclass::text <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update tables {", ".join(failing)} to have a correct primary key.

Primary keys should be defined as:

    id bigint GENERATED ALWAYS AS IDENTITY PRIMARY KEY

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_tables_have_external_ids(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT t.table_name
        FROM information_schema.tables t
        WHERE t.table_type = 'BASE TABLE'
            AND t.table_schema = 'public'
            AND NOT EXISTS (
                SELECT *
                FROM information_schema.columns c
                WHERE c.table_name = t.table_name
                    AND c.column_name = 'external_id'
            )
            AND t.table_name <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update tables {", ".join(failing)} to have an `external_id` column.

External IDs are used externally to avoid leaking information.

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_external_ids_have_not_null_unique(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT t.table_name
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE c.column_name = 'external_id'
            AND t.table_type = 'BASE TABLE'
            AND t.table_schema = 'public'
            AND (
                NOT EXISTS (
                    SELECT *
                    FROM information_schema.table_constraints AS tc
                    JOIN information_schema.constraint_column_usage AS ccu
                    ON ccu.constraint_name = tc.constraint_name
                        AND ccu.table_schema = tc.table_schema
                    WHERE ccu.column_name = c.column_name
                        AND ccu.table_name = t.table_name
                )
                OR c.is_nullable = 'YES'
            )
        """
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update tables {", ".join(failing)} to have a NOT NULL UNIQUE `external_id`.

All rows require a unique external IDs for us to reference them externally.

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_external_id_validation(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.table_name, c.column_default
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE c.column_name = 'external_id'
        AND t.table_schema = 'public'
        AND t.table_type = 'BASE TABLE';
        """
    )
    prefix_regex = re.compile(r"^[a-z]{3}$")

    seen_prefixes = set()
    for table, default in await cursor.fetchall():
        prefix = default.split("'")[1]
        ok = prefix_regex.match(prefix) is not None
        assert ok, f"""\
Table {table} has an invalid external_id prefix. Prefixes must be 3 lowercase characters.
"""
        ok = prefix not in seen_prefixes
        assert ok, f"""\
Table {table} has a duplicate external_id prefix. Please choose a new one.
"""
        seen_prefixes.add(prefix)


async def test_all_timestamptz(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.table_name, c.column_name
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE t.table_type = 'BASE TABLE'
            AND c.table_schema = 'public'
            AND c.data_type = 'timestamp without time zone'
            AND c.table_name || '.' || c.column_name <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [f"{x[0]}.{x[1]}" for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update columns {", ".join(failing)} to be timestamptz (timezone-aware).

Timezone-unaware timestamps are not allowed.

Failing columns:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_tables_have_created_at_and_updated_at(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT t.table_name
        FROM information_schema.tables t
        WHERE t.table_type = 'BASE TABLE'
            AND t.table_schema = 'public'
            AND (
                NOT EXISTS (
                    SELECT *
                    FROM information_schema.columns c
                    WHERE c.table_name = t.table_name
                        AND c.column_name = 'created_at'
                )
                OR NOT EXISTS (
                    SELECT *
                    FROM information_schema.columns c
                    WHERE c.table_name = t.table_name
                        AND c.column_name = 'updated_at'
                )
            )
            AND t.table_name <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update tables {", ".join(failing)} to have created_at and updated_at columns.

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_created_and_updated_at_definition(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.table_name, c.column_name
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE c.column_name IN ('created_at', 'updated_at')
            AND t.table_schema = 'public'
            AND t.table_type = 'BASE TABLE'
            AND (
                c.column_default IS DISTINCT FROM 'now()'
                OR c.is_nullable = 'YES'
            )
        """
    )
    failing = [f"{x[0]}.{x[1]}" for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update columns {", ".join(failing)} to NOT NULL DEFAULT NOW().

Failing columns:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_updated_at_columns_have_trigger(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.table_name
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE c.column_name = 'updated_at'
            AND t.table_schema = 'public'
            AND t.table_type = 'BASE TABLE'
            AND t.table_name NOT IN (
                SELECT event_object_table
                FROM information_schema.triggers
                WHERE trigger_schema = 'public'
                    AND trigger_name = 'updated_at'
                    AND event_manipulation = 'UPDATE'
                    AND action_statement = 'EXECUTE FUNCTION updated_at()'
                    AND action_orientation = 'ROW'
                    AND action_timing = 'BEFORE'
            )
        """
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update tables {", ".join(failing)} to have an updated_at trigger.

Every table should have this trigger:

    CREATE TRIGGER updated_at BEFORE UPDATE ON table_name
    FOR EACH ROW EXECUTE PROCEDURE updated_at();

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_integers_should_be_bigints(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.table_name, c.column_name
        FROM information_schema.columns c
        JOIN information_schema.tables t
            ON c.table_name = t.table_name
        WHERE c.data_type = 'integer'
            AND t.table_schema = 'public'
            AND t.table_type = 'BASE TABLE'
            AND t.table_name <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [f"{x[0]}.{x[1]}" for x in await cursor.fetchall()]
    assert not failing, f"""\
Please update columns {", ".join(failing)} to be of the bigint type.

32-bit integers are at risk of overflowing due to their small size.

Failing columns:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


@dataclass
class MissingFK:
    table: str
    columns: list[str]


async def test_foreign_key_indexes(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        -- Based on https://www.cybertec-postgresql.com/en/index-your-foreign-key/.
        SELECT
            c.conrelid::regclass AS table,
            -- List of key column names in order.
            array_agg(a.attname ORDER BY x.n)::TEXT[] AS columns
        FROM pg_catalog.pg_constraint c
            -- Enumerated key column numbers per foreign key.
            CROSS JOIN LATERAL unnest(c.conkey) WITH ORDINALITY AS x(attnum, n)
            -- Name for each key column.
            JOIN pg_catalog.pg_attribute a ON a.attnum = x.attnum AND a.attrelid = c.conrelid
        WHERE
            -- Is there a matching index for the constraint? A matching index is one that has the
            -- first column set to the foreign key column and is not partial.
            NOT EXISTS (
                SELECT FROM pg_catalog.pg_index i
                WHERE i.indrelid = c.conrelid
                    -- It must not be a partial index.
                    AND i.indpred IS NULL
                    -- The first index columns must be the same as the key columns, but order
                    -- doesn't matter.
                    AND (i.indkey::smallint[])[0:cardinality(c.conkey)-1]
                        OPERATOR(pg_catalog.@>) c.conkey
            )
            -- Is there a matching partial index for this constraint? This only allowed for cases
            -- where a single column is being indexed and the expression is IS NOT NULL. This allows
            -- for more optimized nullable foreign key indexes.
            AND NOT EXISTS (
                SELECT FROM pg_catalog.pg_index i
                WHERE i.indrelid = c.conrelid
                    -- It must have a single indexed column.
                    AND array_length(indkey::smallint[], 1) = 1
                    -- The predicate is IS NOT NULL
                    AND pg_get_expr(i.indpred, i.indrelid) LIKE '(% IS NOT NULL)'
                    -- The first index columns must be the same as the key columns, but order
                    -- doesn't matter.
                    AND (i.indkey::smallint[])[0:cardinality(c.conkey)-1]
                        OPERATOR(pg_catalog.@>) c.conkey
            )
            AND c.contype = 'f'
        GROUP BY c.conrelid, c.conname
        """
    )
    failing = [MissingFK(table=x[0], columns=x[1]) for x in await cursor.fetchall()]
    assert not failing, f"""\
Please add indexes on foreign keys {
    ", ".join([f"{x.table} ({','.join(x.columns)})" for x in failing])
}.

See https://www.cybertec-postgresql.com/en/index-your-foreign-key/ for rationale.

Fixes:

{nl.join([
    f"CREATE INDEX {x.table}_{'_'.join(x.columns)}_idx ON {x.table} ({', '.join(x.columns)});"
    for x in failing
])}
"""  # pragma: no cover


async def test_all_tables_have_row_security_policy(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.relname
        FROM pg_class c
        JOIN pg_namespace n ON n.oid = c.relnamespace
        -- Only check tables in the public namespace.
        WHERE n.nspname = 'public'
            -- Only check "ordinary tables."
            AND c.relkind = 'r'
            -- Check for tables where there is no Row Level Security policy defined.
            AND NOT c.relrowsecurity
            -- Ignore migration tables.
            AND c.relname <> ANY(%s)
        """,
        (IGNORE_TABLES,),
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please add a Row Level Security policy to {", ".join(failing)}.

Row Level Security policies by default restrict customer access to data, and an explicit policy is
required to give them access to that data.

Failing tables:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_views_have_security_invoker_true(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT c.relname
        FROM pg_class c
        JOIN pg_namespace n ON n.oid = c.relnamespace
        -- Only check views in the public namespace.
        WHERE n.nspname = 'public'
            -- Only check relations that are views or materialized views.
            AND c.relkind IN ('v', 'm')
            -- Check for views without security invoker.
            AND NOT ('security_invoker=true' = ANY(COALESCE(c.reloptions, array[]::text[])))
        """
    )
    failing = [x[0] for x in await cursor.fetchall()]
    assert not failing, f"""\
Please define the following views with security invoker: {", ".join(failing)}.

Security Invoker ensures that Row Level Security is enforced based on the user querying the view. By
default, Postgres enforces Row Level Security based on the owner of the view (aka Security Definer).

To enable Security Invoker, please define the view as:

    CREATE VIEW view_name WITH (security_invoker = true) AS ( ... );

Failing views:
{nl.join(f"- {t}" for t in failing)}
"""  # pragma: no cover


async def test_all_user_and_tenants_cascade(t: TFix) -> None:
    cq = await t.db.conn_admin()
    cursor = await cq.c.execute(
        """
        SELECT
			kcu.table_name,
			c.column_name
		FROM information_schema.constraint_column_usage ccu
		JOIN information_schema.referential_constraints rc
			ON ccu.constraint_name = rc.unique_constraint_name
		JOIN information_schema.key_column_usage kcu
			ON kcu.constraint_name = rc.constraint_name
		JOIN information_schema.columns c
			ON kcu.table_name = c.table_name AND kcu.column_name = c.column_name
		WHERE ccu.table_name IN ('users', 'tenants')
			AND rc.delete_rule NOT IN ('CASCADE', 'SET NULL')
        """
    )
    failing = [f"{x[0]}.{x[1]}" for x in await cursor.fetchall()]
    assert not failing, f"""\
Please add ON DELETE CASCADE or ON DELETE SET NULL to the following columns: {", ".join(failing)}.

Upon user or vendor deletion, all rows related to them must either be nulled or deleted. This allow
us to easily delete users and tenants in response to GDPR requests.

Failing columns:
{nl.join(f"- {c}" for c in failing)}
"""  # pragma: no cover
